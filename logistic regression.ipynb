{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the packages\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy Dataset\n",
    "X = np.array([[1, 2], [4, 5], [2, 5]])\n",
    "y = np.array([1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([[3, 1], [3, 5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Logistic Regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        '''\n",
    "        Computes the sigmoid\n",
    "        '''\n",
    "        return 1/(1 + np.exp(-z))\n",
    "    \n",
    "    def loss_function(self):\n",
    "        '''\n",
    "        Computes log loss for binary classification. \n",
    "        In case ofmulti class classigication, we need to compute cross entropy\n",
    "        '''\n",
    "        loss = (-self.y * np.log(self.train_pred + 0.0001) - (1-self.y)*np.log(1-self.train_pred + 0.0001)).mean()\n",
    "        return loss\n",
    "    \n",
    "    def gradient_descent(self):\n",
    "        '''\n",
    "        Updates the weights by computing: W - learning rate * gradient.\n",
    "        gradient = x * (sigmoid(z) - y)\n",
    "        \n",
    "        Returns:\n",
    "        W - Updates weights\n",
    "        '''\n",
    "        grad = np.dot(self.X.T, (self.train_pred - self.y)) * self.learning_rate\n",
    "        self.W = self.W - grad\n",
    "        return self.W\n",
    "    \n",
    "    def log_reg(self, learning_rate, iterations):\n",
    "        '''\n",
    "        Function that iterates over \"iterations\" to compute loss and then \n",
    "        performs gradient descent to update the weights\n",
    "        \n",
    "        Inputs:\n",
    "        learning_rate - learning rate\n",
    "        iterations - Number of iteations\n",
    "        \n",
    "        ''' \n",
    "        self.learning_rate = learning_rate\n",
    "        self.W = np.random.randn(len(self.X[0])) # weights are randomly initialised at first\n",
    "        for it in range(iterations):\n",
    "            self.train_pred = []\n",
    "            for row in self.X:\n",
    "                z = np.sum([row[i] * self.W[i] for i in range(len(row))])\n",
    "                self.train_pred.append(self.sigmoid(z))\n",
    "            self.train_pred = np.array(self.train_pred)\n",
    "            loss = self.loss_function()\n",
    "            print(f'Epoch {it}, loss {loss}')\n",
    "            self.W = self.gradient_descent()\n",
    "        print(f'Coefficients: {self.W}')\n",
    "        \n",
    "    def predict(self, test):\n",
    "        '''\n",
    "        Multiply the features of every row with the corresponding coefficients\n",
    "        '''\n",
    "        self.test_pred = []\n",
    "        for row in test:\n",
    "            z = np.sum([row[i] * self.W[i] for i in range(len(row))])\n",
    "            self.test_pred.append(self.sigmoid(z))\n",
    "        print(f'Predictions: {self.test_pred}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the model\n",
    "model = LogisticRegression(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss 1.1350142379847765\n",
      "Epoch 1, loss 0.5303797437020138\n",
      "Epoch 2, loss 0.4796497048366155\n",
      "Epoch 3, loss 0.46323724890285595\n",
      "Epoch 4, loss 0.4609230721947349\n",
      "Epoch 5, loss 0.4607776617596388\n",
      "Epoch 6, loss 0.46076331529137554\n",
      "Epoch 7, loss 0.4607538648441638\n",
      "Epoch 8, loss 0.4607448184849872\n",
      "Epoch 9, loss 0.4607360427770874\n",
      "Coefficients: [-0.53004575 -0.07845485]\n"
     ]
    }
   ],
   "source": [
    "# Fit logistic regression to get the coefficients\n",
    "model.log_reg(learning_rate=0.1, iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0.15861197983194045, 0.12106200618100489]\n"
     ]
    }
   ],
   "source": [
    "# Predict target values for the test set\n",
    "model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
